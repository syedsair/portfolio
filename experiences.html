<!DOCTYPE HTML>
<html>
	<head>
		<title>Syed Sair Ali Shah - Data Engineer</title>
		<link rel="icon" type="image/x-icon" href="images/logo.svg">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<a href="index.html"><span class="logo"><img src="images/logo.svg" alt="" /></span></a>
						<a href="index.html"><h1>Syed Sair Ali Shah</h1></a>
						<p>
							Data Engineer | Cloud Enthusiast | Data Analyst
							<br>
							Helping companies to make right, data-driven decisions
						</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#experience">Experiences</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
							<!-- Experience -->
							<section id="experience" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Senior Data Engineer at BitFicial <br></h2>
										</header>
										<p>
											<ul>
												<li>
													Built a sophisticated royalty dashboard application from scratch. 
													Leveraging Django as the backend framework and Django's templates for the frontend, 
													I seamlessly integrated various components to create a seamless user experience. 
													The project required handling large volumes of data, necessitating efficient data management techniques. 
													To automate recurring tasks, I implemented cron jobs, ensuring timely updates and data synchronization. 
													Additionally, I utilized JavaScript and Python to enhance the application's functionality and interactivity. 
													Deploying the application on AWS further demonstrated my proficiency in cloud computing and scalability. 
													This project not only challenged me to think critically and creatively, 
													but it also fortified my skills in full-stack development and data handling.
												</li>
												<li>
													Managed data team on a groundbreaking project that entailed the deployment of a machine learning model 
													API and a robust backend. Python served as the primary language for developing the machine learning model 
													API, which was meticulously crafted to deliver accurate predictions and insights. 
													Leveraging my expertise in cloud computing, I successfully deployed both the model API and 
													the backend on the Google Cloud Platform (GCP), ensuring scalability and high availability. 
													To streamline the data flow and processing, I implemented efficient data pipelines on GCP, 
													facilitating seamless integration between the model API and the backend. 
													This project challenged me to apply my skills in Python programming, machine learning, 
													and cloud deployment.
												</li>
												<li>
													Worked on an impactful project that involved data synchronization from various sources using SynHub. 
													I effectively integrated data from platforms such as Xero and Unleashed, ensuring accurate 
													and up-to-date information for analysis. Leveraging my expertise in data manipulation and analysis, 
													I generated valuable insights from the integrated data sets. To effectively communicate these 
													insights to clients, I leveraged Power BI to create visually appealing and informative dashboards. 
													These dashboards provided clients with a comprehensive view of their business performance and key 
													metrics, empowering them to make data-driven decisions. This project allowed me to demonstrate 
													my proficiency in data analysis, data integration, and visualization, showcasing my ability to 
													deliver actionable insights to clients.
												</li>
											</ul>
										</p>
									</div>
									<span class="image"><img src="images/bitficial.jpg" alt="" /></span>
								</div>
								<hr>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Python/Data Engineer at Developers Studio <br></h2>
										</header>
										<p>
											<ul>
												<li>
													Leading data engineering team, I oversaw the collection of diverse data types from multiple sources and unified
													them into a single database. We utilized advanced data integration techniques, including machine learning
													algorithms, to merge, cleanse, and standardize the data. Our efforts resulted in a user-friendly search engine
													that met our specific requirements and provided reliable and accurate information.
												</li>
												<li>
													Optimized the cron jobs and reduced execution times by 30%, enabling the data team to run the jobs every hour
													instead of once a day. This improved data freshness and availability. I analyzed the existing jobs, streamlined the
													process, and eliminated unnecessary steps to create a more efficient workflow.
												</li>
											</ul>
										</p>
									</div>
									<span class="image"><img src="images/ds.png" alt="" /></span>
								</div>
								<hr>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Data Engineer at ML Sense <br></h2>
										</header>
										<p>
											<ul>
												<li>
													Worked with the data science team, and wrote multiple Python/PySpark scripts, to fetch data from different apis, and
											scheduled them to run daily and append the data into Azure Databricks Spark tables.
												</li>
												<li>
													Converted raw JSON data streams, coming into the data lake, into analyzed dashboard in Azure Databricks. Cleaning was
											done by scheduled Python scripts and SQL queries.
												</li>
												<li>
													Created GCP Cloud Audit tool with the help of Cloud Audit logs, BigQuery and Cloud Functions. The tool provides complete
											and precise information about each resource, created in a project. The information contained Resource Name, Creation
											Time, Update Time, Creator (Service Account/User), Project, Cost, Data Processed etc.
												</li>
												<li>
													Created an ETL pipeline using Cloud Functions, Scheduler and Bigquery. This procedure gradually scraped almost 27 million
											records from the source, cleaned incoming data, and then stored it in Cloud SQL on daily basis. This data was then used for
											a dashboard for a client.
												</li>
												<li>
													After in-depth data analysis, designed and created interactive dashboards for multiple clients in Tableau.
												</li>
												<li>
													Used Django and React to create a web application, deployed to GCP. Scheduled data fetch from Facebook API, Trueclicks
											and Google Display and Video (DV 360) and wrote python scripts in Django backend and Cloud Functions to analyze the
											data and display the information on frontend using React charts.
												</li>
												<li>
													Successfully migrated complete infrastructure, code and data of an organization from one GCP project to another. Created
											pipelines for the data to be synchronized across projects, so that data can be shared between them.
												</li>
												<li>
													Debugged and resolved multiple issues in multiple GCP projects for both Production and Development environments.
											Debugged cloud services including Terraform deployments, Cloud Composer, Airflow, Cloud Dataflow, Cloud Functions etc.
												</li>
												<li>
													Using Terraform, Github Actions and GCP (Composer Airflow), created end-to-end Data Pipeline, reading, processing and
											storing almost 7 million records each month. Terraform scripts get triggered by GH Actions, and create the Infrastructure
											i.e., Composer Environment, and then GH Actions update Airflow DAGs.
												</li>
												<li>
													Used Terraform to create Datadog monitors for a DAG, running in Airflow.
												</li>
												<li>
													Migrated a data pipeline that was previously running on GCP's Dataflow in Scala to a Python-based pipeline
													deployed on a GCE instance, saving approximately 6000 GBP per annum. The migration required a deep
													understanding of both Scala and Python, as well as expertise in GCP's infrastructure. We optimized the pipeline's
													performance to handle the same workload as the previous pipeline, without sacrificing speed or accuracy,
													resulting in significant cost savings for our organization.
												</li>
											</ul>
										</p>
									</div>
									<span class="image"><img src="images/mlsense.jpg" alt="" /></span>
								</div>
								<hr>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Python Engineer Intern at ML Sense <br></h2>
										</header>
										<p>
											<ul>
												<li>
													Developed a Django React app and created complete CICD pipeline using GitHub Actions to deploy on Google App Engine.
												</li>
												<li>
													Created multiple Python scrapers using Selenium and BS4, and completed a GUI based application, able to handle all the
scrapers from a single User Interface.
												</li>
											</ul>
										</p>
									</div>
									<span class="image"><img src="images/mlsense.jpg" alt="" /></span>
								</div>
								<hr>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Freelancer at Fiverr <br></h2>
										</header>
										<p>
											<li>
												Worked on various client projects including Scraping, Web development, Machine Learning and Database Designing.
											</li>
										</p>
									</div>
									<span class="image"><img src="images/fiverr.png" alt="" /></span>
								</div>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<div style="justify-content: left;">
							<section>
								<img src="images/signature.png" />
							</section>
						</div>
						<div>
							<section>
								<h2><b>Contact</b></h2>
								<dl class="alt">
									<dt>Address</dt>
									<dd>Islampura &bull; Lahore &bull; Pakistan</dd>
									<dt>Phone</dt>
									<dd>(+92) 320 7341507</dd>
									<dt>Email</dt>
									<dd><a href="mailto:sairsyed2@gmail.com">sairsyed2@gmail.com</a></dd>
								</dl>
								<h2><b>Social</b></h2>
								<ul class="icons">
									<li><a href="https://www.linkedin.com/in/syedsair/" class="icon brands fa-linkedin alt"><span class="label">LinkedIn</span></a></li>
									<li><a href="https://github.com/syedsair" class="icon brands fa-github alt"><span class="label">GitHub</span></a></li>
									<li><a href="mailto:sairsyed2@gmail.com" class="icon fa-brands fa-envelope alt"><span class="label">Mail</span></a></li>
									<li><a href="https://www.facebook.com/sair.shah.908" class="icon brands fa-facebook-f alt"><span class="label">Facebook</span></a></li>
									<li><a href="https://www.instagram.com/s.sairas/" class="icon brands fa-instagram alt"><span class="label">Instagram</span></a></li>
								</ul>
							</section>
						</div>
						
						<p class="copyright">&copy; <a href="mailto:sairsyed2@gmail.com">Syed Sair Ali</a></p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
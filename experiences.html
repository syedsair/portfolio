<!DOCTYPE HTML>
<html>
	<head>
		<title>Syed Sair Ali Shah - Data Engineer</title>
		<link rel="icon" type="image/x-icon" href="images/logo.svg">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<a href="index.html"><span class="logo"><img src="images/logo.svg" alt="" /></span></a>
						<a href="index.html"><h1>Syed Sair Ali Shah</h1></a>
						<p>
							Data Engineer | Cloud Enthusiast | Data Analyst
							<br>
							Helping companies to make right, data-driven decisions
						</p>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#experience">Experiences</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">
							<!-- Experience -->
							<section id="experience" class="main">
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Senior Data Scientist at SAAL.ai <br></h2>
										</header>
										<p>
											<ul>
												<li>
													Collaborated with the data team to collect data into Mongo collections and process it using the ONA model,
													providing a comprehensive analysis of the network. We used FLASK to serve this data to the backend team
													through a RESTful API. Our approach included schema design, workflow development, data transformation,
													cleaning, and validation to streamline the process.
												</li>
												<li>
													Currently working on Dataiku to create workflows for the DS team, which will allow them to easily process new
													datasets without needing to create new recipes. By using the same recipes for new data, we can ensure
													consistency in our data analysis processes.
												</li>
												<li>
													These workflows help gather all the data required for the analysis onto a single platform, improving
													collaboration and reducing the time required for data processing. With the ability to automate and schedule
													these workflows, we can reduce the workload on the DS team and improve overall efficiency.
												</li>
											</ul>
										</p>
									</div>
									<span class="image"><img src="images/saal.png" alt="" /></span>
								</div>
								<hr>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Python/Data Engineer at Developers Studio <br></h2>
										</header>
										<p>
											<ul>
												<li>
													Leading data engineering team, I oversaw the collection of diverse data types from multiple sources and unified
													them into a single database. We utilized advanced data integration techniques, including machine learning
													algorithms, to merge, cleanse, and standardize the data. Our efforts resulted in a user-friendly search engine
													that met our specific requirements and provided reliable and accurate information.
												</li>
												<li>
													Optimized the cron jobs and reduced execution times by 30%, enabling the data team to run the jobs every hour
													instead of once a day. This improved data freshness and availability. I analyzed the existing jobs, streamlined the
													process, and eliminated unnecessary steps to create a more efficient workflow.
												</li>
											</ul>
										</p>
									</div>
									<span class="image"><img src="images/ds.png" alt="" /></span>
								</div>
								<hr>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Data Engineer at ML Sense <br></h2>
										</header>
										<p>
											<ul>
												<li>
													Worked with the data science team, and wrote multiple Python/PySpark scripts, to fetch data from different apis, and
											scheduled them to run daily and append the data into Azure Databricks Spark tables.
												</li>
												<li>
													Converted raw JSON data streams, coming into the data lake, into analyzed dashboard in Azure Databricks. Cleaning was
											done by scheduled Python scripts and SQL queries.
												</li>
												<li>
													Created GCP Cloud Audit tool with the help of Cloud Audit logs, BigQuery and Cloud Functions. The tool provides complete
											and precise information about each resource, created in a project. The information contained Resource Name, Creation
											Time, Update Time, Creator (Service Account/User), Project, Cost, Data Processed etc.
												</li>
												<li>
													Created an ETL pipeline using Cloud Functions, Scheduler and Bigquery. This procedure gradually scraped almost 27 million
											records from the source, cleaned incoming data, and then stored it in Cloud SQL on daily basis. This data was then used for
											a dashboard for a client.
												</li>
												<li>
													After in-depth data analysis, designed and created interactive dashboards for multiple clients in Tableau.
												</li>
												<li>
													Used Django and React to create a web application, deployed to GCP. Scheduled data fetch from Facebook API, Trueclicks
											and Google Display and Video (DV 360) and wrote python scripts in Django backend and Cloud Functions to analyze the
											data and display the information on frontend using React charts.
												</li>
												<li>
													Successfully migrated complete infrastructure, code and data of an organization from one GCP project to another. Created
											pipelines for the data to be synchronized across projects, so that data can be shared between them.
												</li>
												<li>
													Debugged and resolved multiple issues in multiple GCP projects for both Production and Development environments.
											Debugged cloud services including Terraform deployments, Cloud Composer, Airflow, Cloud Dataflow, Cloud Functions etc.
												</li>
												<li>
													Using Terraform, Github Actions and GCP (Composer Airflow), created end-to-end Data Pipeline, reading, processing and
											storing almost 7 million records each month. Terraform scripts get triggered by GH Actions, and create the Infrastructure
											i.e., Composer Environment, and then GH Actions update Airflow DAGs.
												</li>
												<li>
													Used Terraform to create Datadog monitors for a DAG, running in Airflow.
												</li>
												<li>
													Migrated a data pipeline that was previously running on GCP's Dataflow in Scala to a Python-based pipeline
													deployed on a GCE instance, saving approximately 6000 GBP per annum. The migration required a deep
													understanding of both Scala and Python, as well as expertise in GCP's infrastructure. We optimized the pipeline's
													performance to handle the same workload as the previous pipeline, without sacrificing speed or accuracy,
													resulting in significant cost savings for our organization.
												</li>
											</ul>
										</p>
									</div>
									<span class="image"><img src="images/mlsense.jpg" alt="" /></span>
								</div>
								<hr>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Python Engineer Intern at ML Sense <br></h2>
										</header>
										<p>
											<ul>
												<li>
													Developed a Django React app and created complete CICD pipeline using GitHub Actions to deploy on Google App Engine.
												</li>
												<li>
													Created multiple Python scrapers using Selenium and BS4, and completed a GUI based application, able to handle all the
scrapers from a single User Interface.
												</li>
											</ul>
										</p>
									</div>
									<span class="image"><img src="images/mlsense.jpg" alt="" /></span>
								</div>
								<hr>
								<div class="spotlight">
									<div class="content">
										<header class="major">
											<h2>Freelancer at Fiverr <br></h2>
										</header>
										<p>
											<li>
												Worked on various client projects including Scraping, Web development, Machine Learning and Database Designing.
											</li>
										</p>
									</div>
									<span class="image"><img src="images/fiverr.png" alt="" /></span>
								</div>
							</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<div style="justify-content: left;">
							<section>
								<img src="images/signature.png" />
							</section>
						</div>
						<div>
							<section>
								<h2><b>Contact</b></h2>
								<dl class="alt">
									<dt>Address</dt>
									<dd>Islampura &bull; Lahore &bull; Pakistan</dd>
									<dt>Phone</dt>
									<dd>(+92) 320 7341507</dd>
									<dt>Email</dt>
									<dd><a href="mailto:sairsyed2@gmail.com">sairsyed2@gmail.com</a></dd>
								</dl>
								<h2><b>Social</b></h2>
								<ul class="icons">
									<li><a href="https://www.linkedin.com/in/syedsair/" class="icon brands fa-linkedin alt"><span class="label">LinkedIn</span></a></li>
									<li><a href="https://github.com/syedsair" class="icon brands fa-github alt"><span class="label">GitHub</span></a></li>
									<li><a href="mailto:sairsyed2@gmail.com" class="icon fa-brands fa-envelope alt"><span class="label">Mail</span></a></li>
									<li><a href="https://www.facebook.com/sair.shah.908" class="icon brands fa-facebook-f alt"><span class="label">Facebook</span></a></li>
									<li><a href="https://www.instagram.com/s.sairas/" class="icon brands fa-instagram alt"><span class="label">Instagram</span></a></li>
								</ul>
							</section>
						</div>
						
						<p class="copyright">&copy; <a href="mailto:sairsyed2@gmail.com">Syed Sair Ali</a></p>
					</footer>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>